\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{hyperref}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\bigC}[0]{\mathcal{C}}
\begin{document}
\title{
An attempt to connect Shannon's
circuit counting bound with clique detection}

\author{Josh Burdick \\
{\tt josh.t.burdick@gmail.com}}
\maketitle

\begin{abstract}
Shannon's function-counting argument
\cite{shannon_synthesis_1949} showed that some Boolean functions have
exponential circuit complexity, but doesn't provide a specific example
of such a hard-to-compute function. A simple modification of that argument
shows that detecting a randomly-chosen subset of the $k$-vertex cliques in an
$n$-vertex graph requires, on average, $\Omega(n^{k/2})$ NAND gates.
Unfortunately,
this doesn't directly bound the complexity of detecting {\em all} of the cliques; however, it seems like a
possibly related problem.
Here, we attempt to connect this
average-case bound on detecting some
cliques, with the problem of detecting all of them.
\end{abstract}

\newpage

\tableofcontents

This is an attempt to obtain a lower bound on detecting cliques
using NAND gates. Although it seems unlikely to work, hopefully
it will add to the long list of strange things which would happen
if P = NP (FIXME CITE).

\section{A counting bound}
\label{countingBound}

The first component we use is a slight modification
of Shannon's function-counting argument
\cite{shannon_synthesis_1949}.

\subsection{Background: lower bounds from function counting}

It has long been known that computing {\em some} function of a bit-string
requires exponentially large circuits \cite{shannon_synthesis_1949}.
If there are $m$ inputs to a circuit,
then there are $2^{2^m}$ possible functions from the $m$-input bitstring to
a one-bit output. Each of these functions, being different, must have a
different circuit.

Let $f: \{0,1\}^m \rightarrow \{0,1\}$ be a function from bitvectors to bits.
Let $\bigC(f)$ be the circuit
with the fewest unbounded fan-in NAND
gates computing $f$ (with ties broken
arbitrarily), and let $|\bigC(f)| = g$ be the number of gates in
said circuit. (This is standard notation, except for the use
of NAND gates as a basis). 

The
circuit could have at most $gm$ wires from inputs to gates, and ${g \choose 2}$
wires from gates to gates. We can view the possible circuits as a bitmask,
containing a 1 everywhere a gate is connected to an input (or another gate),
and 0 everywhere else.

\begin{thm}
\label{boundFromCounting}
Consider functions from $m$ bits to one bit of output.
This means that, with $g$ gates, we can represent at most
$2^{gm + {g \choose 2}}$ different boolean functions (with $m$ bits of input,
and one bit of output).
\end{thm}
\begin{proof}

The number of possible wires which are there, or not, is $gm + {g \choose 2}$,
which bounds how many possible circuits there are.
Some of these circuits compute the same function.
However, there can't be any more than this many circuits with this many wires.
\end{proof}

This means that if we have a large set of functions, and we know the size of
the set of functions, then we know that at least {\em one} of them requires
a large number of gates. (Knowing {\em which} function requires a lot, or many,
gates is still an issue).

Consider functions from $m$ bits to one bit of output.
Let $g$ be the number of gates, and $w$ be the number of wires.
Solving for the number of gates:

\begin{eqnarray*}
w & = & mg + {g \choose 2} \\
  & = & mg + g(g-1)/2 \\
  & = & mg + (g^2 - g) / 2 \\
  & = & 0.5g^2 + (m-0.5)g \\
0 & = & 0.5g^2 + (m-0.5)g - w \\
\end{eqnarray*}

We solve the quadratic formula (writing $b = m-0.5$ for simplicity), keeping
only the non-imaginary root.

\begin{eqnarray*}
g & = & -b \pm \sqrt{ b^2 + 2w} \\
  & = & {\sqrt {2w + b^2}} - b \\
\end{eqnarray*}

Thus, given a set of functions, we know that at least one of them requires
some number of gates.

\subsection{Bounding the average number of gates}

We can also count the total number of functions from $m$ input bits to one
output bit, using up to $g$ NAND gates, as

\begin{eqnarray*}
\sum_{i=0}^{g-1} 2^{m+i} & = & 2^{m+g} - 2^m
\end{eqnarray*}

If we're counting circuits with up to $g$ gates, then some of the circuits
have fewer than $g$ gates. This somewhat complicates the book-keeping.
However, {\em most} of the
circuits have $g$ gates. (Indeed, well over half, since each additional
gate adds many potential wires). Because of this, I think that the
average case bound is just one fewer gates than the worst-case bound.

\subsection{Counting CLIQUE-like functions}

Suppose we are given an $n$-vertex graph.
Let $k$-CLIQUE($n$) be the boolean function which
detects $k$-cliques: it outputs 1 if any $k$-clique
is present, and 0 otherwise. This is a classic
NP-complete problem.

We now consider ``buggy'' NAND gate circuits (with any fan-in) which find some of the possible $k$-cliques in $n$-vertex
graphs.
As a concrete example,
consider a set of ``buggy'' 6-clique finders. 
Maybe the circuit correctly
finds all the cliques. Or maybe it finds all of the cliques except $K_{1..6}$,
or it misses half the cliques, or finds none (and always outputs 0), or maybe
it only successfully finds $K_{1,3,4,5,7,8}$, et cetera.

First, we define a variant of $k$-CLIQUE which only
finds a subset of cliques. Throughout this section,
we will let $K$ denote the set of all possible
$k$-vertex cliques.

\begin{defn}
\label{CLIQUE}
Let $A \subset K$.
CLIQUE$(A)$ is the set of functions which recognize any of the $K_k$s in $A$. That is, for each set $A$ of $K_k$s, CLIQUE$(A)$
contains a function which is 1 if the input contains any $K_k \in A$,
and 0 otherwise. (Using this nomenclature,
$k$-CLIQUE(n) = $CLIQUE(K)$).
\end{defn}

We now define a set of functions:

\begin{defn}
\label{BUGGY-CLIQUE}
Again, let $A \subseteq K$. Define

\[
BUGGY-CLIQUE(A) = \bigcup_{B \subseteq A} CLIQUE(B)
\]
\end{defn}



Of course, many of these functions are quite similar (e.g. all but one of them
output a 1 when you feed in all 1's). However, they're all slightly different.

\begin{thm}
\label{buggyDistinct}
BUGGY-CLIQUE$(A)$ contains $2^{|A|}$ distinct functions.
\end{thm}
\begin{proof}

Let $A,B \subset K$, with A and B different, and w.l.o.g.
let $x \in A-B$. Then $CLIQUE(A)$ outputs 1 on the input
with just the edges in $x$ set to 1 (and 0 everywhere else),
while $CLIQUE(B)$ outputs a 0.

There are $2^{|A|}$ many subsets of the $K_k$s there are,
and by the above, each induces a distinct function.
\end{proof}

For a given $A$, many of the functions in $BUGGY-CLIQUE(A)$
are similar (for instance, most
of them output a 1 when all the edges are present);
but they are all distinct.
Although $2^{n \choose k}$ is a fairly large number,
it's still comfortably less than $2^{2^{n \choose 2}}$, the number of boolean
functions on the ${n \choose 2}$ input wires (one per edge).

\subsubsection{But {\em which} function requires many gates?}

Thus, there are $2^{n \choose k}$ different functions. 
How many NAND gates do these take?
(We consider NAND gate circuits (with any fan-in) which find $k$-cliques in $n$-vertex
graphs, as a circuit with $n \choose 2$ inputs)

Applying Theorem
\ref{boundFromCounting}, we know that at least one of the circuits requires
${\sqrt {2 {n \choose {k/2}} + b^2}} - b = \Omega(n^{k/2})$ 
NAND gates (where $b = {k \choose 2} - 0.5$).

Why doesn't this bound $k$-CLIQUE?
Because we don't know that the circuit which finds {\em all} of the
$K_k$s, is one of these larger circuits. As far as what I've
shown thus far goes, it could be harder to find some weird subset of the $K_k$s.

Indeed, as far as what we've formally shown goes, the problem which needs
the most NAND gates could be finding a single $K_k$! That's easily ruled out
(because that only needs one NAND gate, plus the output gate).

\subsection{Which sets of cliques are hard to find?}
\label{sec:whichCliques}

The hardness of these functions depends
on how the cliques they find are laid out.

Cliques are arguably difficult to draw in a two-dimensional space.
As an approximate diagram reflecting what we know,
we sketch a Hasse of possible subsets of cliques. Although
we only draw a few subsets of three-vertex cliques
on six vertices, hopefully this provides some
intuition.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{R/Hasse.pdf}
\caption{Hasse diagram of BUGGY-CLIQUE functions.
(a-d) 
Detecting all the possible cliques in larger graphs will be
increasingly difficult (although {\em how much} harder isn't clear).
(e) 
Detecting this set of cliques is definitely harder than (b),
since we can convert from (e) to (b) by feeding in 0's to
some set of edges.
(f) Detecting a set of cliques which doesn't overlap much will be
harder than detecting the same number of cliques, when they overlap
maximally (as in (b)).}
\label{fig:Hasse}
\end{figure}


\begin{thm}
\label{edgeZonking}
Let $A \subsetneq B \subseteq K$, such that $B$ covers input edges
which $A$ doesn't. Then $|\bigC(B)| > |\bigC(A)|$.
\end{thm}
\begin{proof}
Feed in a 0 to an edge which is in $B$ but not $A$. At least one
NAND gate will only output a 1.
\end{proof}

This shows that sometimes, finding a larger set of cliques is
harder. However, the above theorem doesn't help if the two
sets of cliques cover the same set of edges.

Triangles can be detected using matrix multiplication \cite{itai_finding_1977},
and there are fast algorithms known for matrix multiplication
\cite{strassen_gaussian_1969}
\cite{williams_multiplying_2012}, so the set of all possible
cliques on some set of vertices
 can be detected
using fewer than one NAND gate per triangle (for large enough input graphs).

On the other hand, if the triangles overlap less (as on the right),
then to detect all of the triangles, we will definitely need at least one
gate per triangle.
To see this, consider feeding in a 0 to the input for one
of the edges unique to some triangle (this is a
standard technique in circuit bounds).
Then any gate connected to
that edge will only output a constant 1. We can repeat this for each of the
triangles, constructing a series of strictly smaller circuits, each of which 
detects some set of cliques. (Thus, for large enough graphs,
graphs analogous to \ref{fig:Hasse} (f), which don't overlap much,
require more NAND gates to detect than graphs 
analogous to (b), which overlap
a lot).

\subsection{Defining levels of CLIQUE}

An intuitive measure for how hard a set of cliques
is to find is, simply, the number of cliques.

As alluded to above, we know that there's at least 


\begin{defn}
\label{CLIQUE-level}
Assume $n, k$ are fixed. Let $K_l$ be the set of all sets
with exactly $l$ cliques. 
\end{defn}

The definitions of $CLIQUE(K_l)$ and $\bigC(CLIQUE(K_l))$ then
follow obviously.

\subsubsection{Concavity}

What can we say about $E[\bigC(CLIQUE(K_l))]$? If we could
prove something in general, for all levels $l$, then we'd be bounding
$CLIQUE = CLIQUE(K_N)$.


\begin{thm}
\label{concavity}
Pick $a, b$ with $0 < a < b < a+b < N$. We have:
\[
E[|\bigC(CLIQUE(K_{a+b}))] \le
E[|\bigC(CLIQUE(K_a))|] + E[|\bigC(CLIQUE(K_b))|]
\]

In other words, $E[|\bigC(CLIQUE(K_l))|]$ is ``concave downward''.
\end{thm}
\begin{proof}
Consider a random set in $K_{a+b}$. Break it randomly into
two sets, which are in $K_a$ and $K_b$, and pick minimal
circuits which find each of these. We can then OR them together
(using no additional gates -- FIXME is this true?), and end up
with a circuit for the edges in $K_{a+b}$

All the choices there are arbitrary, and so we're overcounting
the circuits in $K_a$ and $K_b$. But we're overcounting them all
the same number of times.
\end{proof}

This is the sort of proof which combinatorialists (sp?)
might like, and is all well and good. But
it is unfortunately not helpful in
terms of proving CLIQUE is difficult.
It seems intuitive that, in some sense, finding more cliques should
be harder.
Indeed, since we're using NAND gates, we know that finding any non-empty
subset of cliques is strictly harder than finding {\em some} other 
smaller set of cliques \ref{edgeZonking}.


\section{Counting slightly larger sets of functions}

We can construct somewhat larger sets of functions, based on
which cliques they recognize. For instance, we can define the
function PICKY-CLIQUE.

FIXME make A and B disjoint sets?

\begin{defn}{pickyClique}
Let $A$ be a set of cliques, and $B \subset A$.
Define PICKY-CLIQUE(A, B) to be the function which is 1 iff

\begin{itemize}

\item any clique in $A-B$ is present, {\em and}

\item no clique in $B$ is present

\end{itemize}
Cliques outside of $A$ are ignored.

\end{defn}

Note that when B is almost as large as A,
this almost corresponds to the complement of
$k$-CLIQUE (that is, detecting whether a graph
{\em doesn't} contain a clique; not to be
confused with the independent set problem).

\begin{thm}
\label{pickyDistinct}
PICKY-CLIQUE$(A)$ contains $3^{|A|} - 2^{|A|}$ distinct functions.
\end{thm}
\begin{proof}




\end{proof}



\begin{figure}
\centering
\includegraphics[width=1\textwidth]{R/HasseWithOmissions.pdf}
\caption{Hasse diagram of PICKY-CLIQUE functions. 
}
\label{fig:Hasse}
\end{figure}


We can represent this using something akin
to figure \ref{fig:Hasse}. In this case, however, we
draw functions at the level of how many
cliques force the output to be 0 (that is,
the cliques in $B$).

One notable feature of this is that when $A$ is
larger, PICKY-$k$-CLIQUE includes functions
throughout the Hasse diagram. This means that,
intuitively, the average-case complexity of
PICKY-$k$-CLIQUE is weighted towards cases
in which $A$ is larger.

\subsection{Levels of PICKY-CLIQUE}

One obvious way of implementing any function in PICKY-CLIQUE
is by using a circuit to find everything in $A$, and then
masking it with everything in $B$.

\[
E[|\bigC(PICKY-CLIQUE(A, B))|] \le
E[|\bigC(CLIQUE(A))|] + E[|\bigC(CLIQUE(B))|]
\]

Turning this around, we have:

\[
E[|\bigC(CLIQUE(A))|]
\ge
E[|\bigC(PICKY-CLIQUE(A, B))|] - E[|\bigC(CLIQUE(B))|]
\]

Note that we can also use a circuit for $CLIQUE(A \cup B)$,
and then mask out everything in $B$.

It's not clear that this helps, though.

\section{Related work}

This strategy relies heavily on a modification of Shannon's original
function-counting argument \cite{shannon_synthesis_1949}.

Broadly speaking, the idea of using an upper bound to prove a lower bound
is not new. Aaronson describes this as ``ironic complexity theory''
\cite{aaronson_pnp}, and mentions several recent applications of it.

A related question is whether problems
(such as $k$-SAT) are
hard on average \cite{bogdanov2006average}.
These efforts seem to focus more on whether
random
instances of a given problem are hard, rather
than using random problems to show that
a specific problem is hard.

\subsection{Possible relevance to other problems}

Here we sketch other potential applications of this strategy
({\em if} it worked; thus far it doesn't seem to).

\subsubsection{The complement of NP: co-NP}

Although there's no obvious reduction from co-NP to NP, essentially
the same argument seems to work. Suppose some family of circuits
checks that there {\em isn't} any $K_k$ in an $n$-vertex graph.
Then we can construct circuits analogous to BUGGY-CLIQUE.

(FIXME is this right?)

\subsubsection{Quantum computation: BQP}

This lower-bound strategy also seems potentially
relevant to quantum computing,
as the argument makes few restrictions on the sort of gates used.
If it's the case that any function in BQP can be represented
as a circuit made of discrete quantum gates, as well as AND and NOT,
then clique detection isn't in BQP.
However, it's not clear what sort of quantum gates would be
appropriate.

\section{Conclusion}

We give a lower bound on finding {\em some} set of cliques.
It is a modified form of Shannon's counting argument
\cite{shannon_synthesis_1949}. Unfortunately,
this doesn't seem directly related to the
complexity of 

\section{Acknowledgements}

The author would like to thank William Gasarch for introducing him
to lower bound strategies and probabilistic proofs about graphs.
He would also like to thank the maintainers of
several entertaining and relevant blogs, including but
not limited to: the Computational Complexity blog
(Lance Fortnow and William Gasarch), 
G\"odel's Lost Letter (Richard Lipton and Ken Regan),
and Shtetl-Optimized (Scott Aaronson). 
He would
also like to thank the Cheung lab for
employment, and his parents for support
(financial and otherwise).

\bibliography{references}
\bibliographystyle{abbrv}

\end{document}

